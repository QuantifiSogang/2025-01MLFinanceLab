{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mutual Information\n",
    "\n",
    "Mutual Information은 $Y$값을 알 때 초래되는 $X$의 불확실성 감소로 정의된다. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(X,Y) &= H(X) - H(X|Y) \\\\\n",
    "       &= H(X) + H(Y) - H(X, Y) \\\\\n",
    "       &= \\sum_{x \\in S_X} \\sum_{y \\in S_Y} p(x,y)\\log \\left( \\frac{p(x,y)}{p(x)p(y)}\\right) \\\\\n",
    "       &= D_{KL}\\left[ p(x,y) \\Vert p(x)p(y)\\right] = \\sum_{y \\in S_Y} p(y) \\sum_{x \\in S_X} p(x|y) \\log \\left( \\frac{p(x|y)}{p(x)}\\right) \\\\\n",
    "       &= E_Y \\left[ D_{KL}\\left[ p(x|y) \\Vert p(x)\\right] \\right] = \\sum_{x \\in S_X} p(x) \\sum_{y \\in S_Y} p(x|y) \\log \\left( \\frac{p(y|x)}{p(y)}\\right) \\\\\n",
    "       &= E_X \\left[ D_{KL}\\left[ p(y|x) \\Vert p(y)\\right] \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "위에서 $I(X,Y) \\geq 0, I(X,Y) = I(Y,X)$이고, $I(X, X) = H(X)$임을 알 수 있다. $X$와 $Y$가 독립일 때 $p(x, y) = p(x)p(y)$이므로 $I(X, Y) = 0$이다. 상계는 $I(X, Y) \\leq \\min \\{H(X), H(Y)\\}$로 주어진다. 그러나 상호 정보는 거리 척도가 아니다. 왜냐하면 이는 삼각 부등식 $I(X,Z) \\nleq I(X,Y) + I(Y, Z)$을 성립하지 못하기 때문이다. 상호 정보의 중요한 속성은 다음의 그룹화 특성이다.\n",
    "\n",
    "$$I(X, Y, Z) = I(X, Y) + I\\left[(X, Y), Z\\right]$$\n",
    "\n",
    "여기서 $(X, Y)$는 $X$와 $Y$의 결합 분포를 나타낸다. $X, Y$와 $Z$는 그 자체로 결합 분포를 나타낼 수 있기 때문에 위의 특성은 상호 정보를 더 단순한 구성 요소로 분해하는 데 사용할 수 있다. 이는 상호 정보를 응집형 군집화 알고리즘과 전방 특성 선택의 맥락에서 유용한 유사도 척도로 만든다.\n",
    "\n",
    "차원당 일정 수의 분할을 가진 정규화된 그리드로 이산화된 동일 크기의 2개의 배열 $x$와 $y$가 주어졌을 때 아래의 코드는 한계 엔트로피, 결합 엔트로피, 조건부 엔트로피와 상호 정보를 계산하는 법을 보여준다."
   ],
   "id": "fc5cef9b5adcb72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Variational Information\n",
    "\n",
    "정보 변분은 다음과 같이 정의된다\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "VI(X,Y) &= H(X|Y) + H(Y|X) \\\\\n",
    "    &= H(X) + H(Y) - 2I(X,Y) \\\\\n",
    "    &= 2H(X,Y) - H(X) - H(Y) \\\\\n",
    "    &= H(X, Y) - I(X, Y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "이 척도는 다른 변수의 값을 들었을 때 다른 한 변수에서 예상하는 불확실성으로 해석할 수 있다. 이는 $VI(X, Y) = 0 \\Leftrightarrow X = Y$와 같은 하계, $VI(X,Y) \\leq H(X,Y)$와 같은 상계를 갖는다. 정보 변분은 거리 척도다. 왜냐하면 비음성, 대칭성, 삼각 부등식의 공리를 만족하기 때문이다.\n",
    "\n",
    "$H(X, Y)$는 $S_X$와 $S_Y$의 크기의 함수이므로 $VI(X,Y)$는 엄격한 상계를 갖지 않는다. 우리는 여러 모집단 크기 간에 정보 변분을 비교하고자 하기 때문에 이것은 문제가 된다. 다음 식은 모든 $(X,y)$쌍에 대해 0과 1 사이로 한정되는 척도다\n",
    "\n",
    "$$\\tilde{VI}(X,Y) = \\frac{VI(X,Y)}{H(X,Y)} = 1 - \\frac{I(X,Y)}{H(X,Y)}$$\n",
    "\n",
    "Kraskov(2008)을 따라서 더 강한 대안의 한정된 척도는 다음과 같다.\n",
    "\n",
    "$$\\tilde{\\tilde{VI}}(X,Y) = \\frac{\\max\\{H(X|Y), H(Y|X)\\}}{\\max\\{H(X), H(Y)\\}} = 1 - \\frac{I(X,Y)}{\\max\\{H(X), H(Y)\\}}$$\n",
    "\n",
    "여기서 모든 $(X,y)$쌍에 대해 $\\tilde{\\tilde{VI}}(X,Y) \\leq \\tilde{VI}(X,Y)$이다. 이전의 예저를 따라서 아래 코드는 정보 변분을 계산한다."
   ],
   "id": "307befe1ab3163de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e1ad80babbba9e7a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
